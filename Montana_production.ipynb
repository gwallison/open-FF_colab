{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a75320bb-dfb7-48f3-a1ad-bbc0e43bb108",
   "metadata": {},
   "source": [
    "# Making a data set for Montana production\n",
    "\n",
    "## Task\n",
    "Combine and reformat 2 large files of Well information and Well production.\n",
    "\n",
    "## Technical issues encountered\n",
    "- Loading large files into Colab\n",
    "- Handling \"tab\" delimited data\n",
    "- Handling some formatting errors\n",
    "- Having the appropriate data for the task\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6d7226-d1e9-4336-afb4-300a525e880c",
   "metadata": {},
   "source": [
    "## Reading files\n",
    "Two files are needed: \n",
    "- the first is the raw production values and is stored in a zip file on [Montana servers](http://www.bogc.dnrc.mt.gov/production/).  The code below will download that zipfile and extract the needed data.\n",
    "- the second is the well information and **you** will need to gather it from the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06786946-14fa-4126-9e03-1e8617e10c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile \n",
    "import requests\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c5dc89-f30f-4e8a-9d5c-6e6c6e76e1e2",
   "metadata": {},
   "source": [
    "### Download production zip file from Montana server\n",
    "Downloading this zipfile (over 140Mbyte) can take **two minutes or more** due to an apparently slow server in Montana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df4b841-d026-4155-bffc-804f5235c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an efficient routine to fetch a large file from a web address and save it\n",
    "# https://stackoverflow.com/a/39217788/6736072\n",
    "\n",
    "def download_file(url):\n",
    "    local_filename = url.split('/')[-1] # just names the local file like the last part of the link\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "    print(f'Downloaded {local_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19f5a5b-675f-4876-aef7-9f195360787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "zfn = 'http://www.bogc.dnrc.mt.gov/production/historical.zip'\n",
    "download_file(zfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cdde46-ea73-4fc7-89ea-a83f29eb3c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now pull the well production file into a dataframe\n",
    "# Note that in these \"read_csv\" functions, we set \"sep\" to  \" \\t \" which is a TAB character.\n",
    "\n",
    "with zipfile.ZipFile(zfn.split('/')[-1]) as z:\n",
    "    with z.open('histprodwell.tab') as f:\n",
    "        prod = pd.read_csv(f,sep='\\t',  # it is TAB delimited\n",
    "                           low_memory=False,\n",
    "                           dtype={'API_WELLNO':'str'}) # we need to treat as a string not a number\n",
    "\n",
    "prod['year'] = pd.to_datetime(prod.rpt_date).dt.year # keep just the year\n",
    "\n",
    "# Keep only fields that we need and rename them to something more useful\n",
    "prod = prod[['API_WELLNO','BBLS_OIL_COND', 'MCF_GAS', 'BBLS_WTR', 'DAYS_PROD', 'year']]\n",
    "prod.columns = ['APINumber','Oil', 'Gas', 'Water','Days','year']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba88e99c-c18c-4145-b9e8-5ec7ba7a7207",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = prod.groupby(['APINumber','year'],as_index=False)[['Oil', 'Gas', \n",
    "                                                        'Water','Days']].sum()\n",
    "gb.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad950b08-faa7-4eaa-a407-0142a36357b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = ['Oil','Gas','Water','Days']\n",
    "concat_list = []\n",
    "for col in colnames:\n",
    "    piv = gb.pivot(index='APINumber',columns='year',values=col).fillna(0)\n",
    "    names = piv.columns.tolist()\n",
    "    ncols = []\n",
    "    for name in names:\n",
    "        ncols.append(col+'_'+str(name))\n",
    "    piv.columns = ncols\n",
    "    concat_list.append(piv)\n",
    "\n",
    "whole = pd.concat(concat_list,axis=1)\n",
    "\n",
    "whole.to_csv('piv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a28a00-c109-4333-8623-cfa811431f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the two files below come from the site: http://www.bogc.dnrc.mt.gov/production/\n",
    "#  in the historical zip file.\n",
    "\n",
    "prod = pd.read_csv(r\"C:\\MyDocs\\OpenFF\\data\\non-FF\\montana\\histprodwell.tab\",sep='\\t',\n",
    "                   low_memory=False,\n",
    "                  dtype={'API_WELLNO':'str'}) # we need to treat as a string not a number\n",
    "\n",
    "prod['date'] = pd.to_datetime(prod.rpt_date) # get the date into a pandas datetime format\n",
    "\n",
    "# Note that in the next line, encoding is explicitly given.  This is because without that, an\n",
    "#  error was thrown.  This solution was found at:\n",
    "#  https://stackoverflow.com/questions/18171739/unicodedecodeerror-when-reading-csv-file-in-pandas-with-python\n",
    "\n",
    "well = pd.read_csv(r\"C:\\MyDocs\\OpenFF\\data\\non-FF\\montana\\histWellData.tab\",sep='\\t',low_memory=False,\n",
    "                  encoding = \"ISO-8859-1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2ec56b-5126-46d1-b5cf-b82b13d25d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "well.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4715a8df-6041-4537-822e-89a22bd2684d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the location data comes directly from the commission's public website:\n",
    "# but note that we have saved the data using their \"text\" button and saved the file with a CSV extention.\n",
    "#   The data are TAB delimited.\n",
    "loc = pd.read_csv(r\"C:\\MyDocs\\OpenFF\\data\\non-FF\\montana\\location.csv\",sep = '\\t')\n",
    "\n",
    "# Show how long each data frame is:\n",
    "print(f'Len: prod: {len(prod)}, well: {len(well)}, loc: {len(loc)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83016433-3199-45f3-abab-2342b9d72801",
   "metadata": {},
   "source": [
    "## Clean up names and APINumbers values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ccdd7e-f0bd-4993-b93a-a486e75597dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod['APINumber'] = prod.API_WELLNO\n",
    "loc['APINumber'] = loc['API #'].str.replace('-','')\n",
    "lapis = loc['APINumber'].unique().tolist()\n",
    "papis = prod['APINumber'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf6e5e5-9afb-47e6-b1fe-83d9875ad3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  let's look at the production data for ONE well (say #1000 in our list)\n",
    "prod[prod.APINumber==papis[1000]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2015fd88-b7ac-46d8-847f-20ca18ebf8a4",
   "metadata": {},
   "source": [
    "This shows that there are more than 300 rows of data for this well.  The data appear to be reported monthly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577dd31c-2afd-4367-9261-e7975d4ff242",
   "metadata": {},
   "source": [
    "## Summarize to single value for all wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13994645-6677-48e2-8e29-44cca29f2320",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = prod.groupby('APINumber',as_index=False)[['BBLS_OIL_COND', 'MCF_GAS', 'BBLS_WTR', 'DAYS_PROD']].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d929291d-6adc-4b38-8e57-35c6d38fbc46",
   "metadata": {},
   "source": [
    "### and add lat/lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ba389d-4514-403b-943e-08f325cca2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mg = pd.merge(gb,loc[['APINumber', 'Wh_Long', 'Wh_Lat']],on='APINumber',how = 'left')\n",
    "mg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bf514b-5e6c-490b-bd3a-333fa9e3828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mg.to_csv('./tmp/MT_prod_summary.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
